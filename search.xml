<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>本地部署私人化大模型</title>
      <link href="/2024/03/18/Minimind%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/"/>
      <url>/2024/03/18/Minimind%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/</url>
      
        <content type="html"><![CDATA[<h1 id="本地部署私人化大模型"><a href="#本地部署私人化大模型" class="headerlink" title="本地部署私人化大模型"></a>本地部署私人化大模型</h1><p><em>本项目基于<a href="https://github.com/jingyaogong/minimind">Minimind</a>开源 ，以Minimind模型为例进行微调。</em> </p><h2 id="0-大模型"><a href="#0-大模型" class="headerlink" title="0.大模型"></a>0.大模型</h2><p>&emsp;&emsp;大语言模型，一种比较学术的定义是：具有大规模参数和复杂计算结构的机器学习语言模型。别看表述得有点难度，其实我们可以从字面上理解，无非就是大号的语言模型。<br>&emsp;&emsp;语言模型就像是一个理解语言规则和用法的智能系统。它能够分析文本数据，并学习其中的语言模式，例如词汇的使用方式、语法结构和句子的连贯性。这种模型可以用来预测一个句子中下一个可能的词语是什么，或者根据给定的文本生成新的文本。你可能听说过，大模型其实就是“预测下一个词”。本质一点地说，就是：这个模型，对于任意的词序列，能够计算出这个序列是“一句人话”的概率。<br><img src="/image/1.png" alt="Embedding表示"><br>&emsp;&emsp;这里举个例子：现在，假设我们有一个句子的部分序列，例如：”我喜欢吃”。我们想要预测下一个可能的词是什么。这时，我们可以利用语言模型给出的概率信息。语言模型告诉我们，“我喜欢吃”这个序列后面接上“苹果”的概率是0.3，“香蕉”的概率是0.2，“西瓜”的概率是0.1，等等。然后，通过比较这些概率，我们可以选择概率最高的词作为下一个词的预测结果。在这个例子中，由于“苹果”的概率最高，我们就可以预测下一个词可能是“苹果”。</p><h2 id="1-Minimind介绍"><a href="#1-Minimind介绍" class="headerlink" title="1.Minimind介绍"></a>1.Minimind介绍</h2><p>&emsp;&emsp;此开源项目旨在完全从0开始，仅用3块钱成本 + 2小时！即可训练出仅为25.8M的超小语言模型MiniMind。<br>&emsp;&emsp;MiniMind系列极其轻量，最小版本体积是 GPT-3 的 17000，力求做到最普通的个人GPU也可快速训练。<br>&emsp;&emsp;项目同时开源了大模型的极简结构-包含拓展共享混合专家(<em>MoE</em>)、数据集清洗、预训练(<em>Pretrain</em>)、监督微调(<em>SFT</em>)、<em>LoRA</em>微调， 直接偏好强化学习（<em>DPO</em>）算法、模型蒸馏算法等全过程代码。</p><p><strong>我的显卡配置：NVIDIA GeForce RTX 2060</strong></p><h2 id="2-项目内容介绍"><a href="#2-项目内容介绍" class="headerlink" title="2.项目内容介绍"></a>2.项目内容介绍</h2><p>&emsp;&emsp; MiniMind-LLM* 结构的全部代码（<em>Dense+MoE</em> 模型）。<br>&emsp;&emsp;包含Tokenizer分词器详细训练代码。<br>&emsp;&emsp; 包含<em>Pretrain</em>、<em>SFT</em>、<em>LoRA</em>、<em>RLHF-DPO</em>、模型蒸馏的全过程训练代码。<br>&emsp;&emsp;收集、蒸馏、整理并清洗去重所有阶段的高质量数据集，且全部开源。<br>&emsp;&emsp;从0实现预训练、指令微调、<em>LoRA</em>、<em>DPO</em> 强化学习，白盒模型蒸馏。关键算法几乎不依赖第三方封装的框架，且全部开源。<br>&emsp;&emsp; 同时兼<em>transformers</em>、<em>trl</em>、<em>peft</em>等第三方主流框架。<br>&emsp;&emsp;训练支持单机单卡、单机多卡(<em>DDP</em>、<em>DeepSpeed</em> )训练，支持<em>wandb</em>可视化训练流程。支持动态启停训练。<br>&emsp;&emsp; 在第三方测评榜（<em>C-Eval</em>、<em>C-MMLU</em>、<em>OpenBookQA</em>等）进行模型测试。<br>&emsp;&emsp; 实现<em>Openai-Api</em>协议的极简服务端，便于集成到第三方<em>ChatUI <em>使用（</em>FastGPT</em>、<em>Open-WebUI</em>等）。<br>&emsp;&emsp; 基于<em>streamlit</em> 实现最简聊天<em>WebUI</em> 前端。<br>&emsp;&emsp; 复现(蒸馏&#x2F;<em>RL</em>)大型推理模型<em>DeepSeek-R1</em>的<em>MiniMind-Reason</em> 模型，数据+模型全部开源！</p><h2 id="3-预训练-pretrain"><a href="#3-预训练-pretrain" class="headerlink" title="3.预训练(pretrain)"></a>3.预训练(pretrain)</h2><p>&emsp;&emsp;预训练模型是机器学习领域的一种重要方法，其核心思想是在大量未标注的数据上进行初步训练（即预训练），以学习数据的通用特征和规律。<br>&emsp;&emsp;这些学习到的特征随后可以应用于各种特定任务，通过在少量标注数据上进行进一步的精调（微调），使得模型能够快速适应并提高在这些特定任务上的表现。具体来说，预训练模型首先在一个原始任务上进行训练，这个原始任务通常设计得让模型能够学习到数据的广泛（说白了就是让机器会词语接龙，会说人话）和普遍的特征。<br><strong>首先要下载好相关的依赖：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">pip install requirements<br></code></pre></td></tr></table></figure><p><strong>Hugging face上下载好相关的模型：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">git clone https://huggingface.co/jingyaogong/MiniMind2<br></code></pre></td></tr></table></figure><p><img src="/image/12.png" alt="预训练前对话1"><br><img src="/image/3.png" alt="预训练前对话2"><br><img src="/image/4.png" alt="预训练前对话3"><br><img src="/image/5.png" alt="预训练前对话4"><br><img src="/image/6.png" alt="预训练前对话5"><br><img src="/image/7.png" alt="预训练前对话6"><br>&emsp;&emsp;<strong>我们可以看到目前模型的理解能力还很低，答非所问，胡言乱语，貌似是答的人话，但其实毫无作用。</strong><br>&emsp;&emsp;<strong>故我们要进行预训练文本。</strong><br><img src="/image/%E9%A2%84%E8%AE%AD%E7%BB%831.png" alt="预训练文本"><br><strong>训练命令：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">python train_pretrain.py<br></code></pre></td></tr></table></figure><p><img src="/image/%E8%AE%AD%E7%BB%831.png" alt="预训练过程"><br><strong>预训练后测试：</strong><br><img src="/image/%E9%A2%84%E8%AE%AD%E7%BB%83%E5%90%8E1.png" alt="预训练测试"><br><img src="/image/%E9%A2%84%E8%AE%AD%E7%BB%83%E5%90%8E2.png" alt="预训练测试"><br><strong>我们可以看到黑色命令框的回答同一个问题的答案要比另一个要好（这里做了一个预训练和未训练的对比）</strong></p><h2 id="4-监督学习-SFT"><a href="#4-监督学习-SFT" class="headerlink" title="4.监督学习(SFT)"></a>4.监督学习(SFT)</h2><p>&emsp;&emsp;SFT (Supervised Fine-Tuning)，经过预训练，LLM此时已经掌握了大量知识，然而此时它只会无脑地词语接龙，还不会与人聊天。 SFT阶段就需要把半成品LLM施加一个自定义的聊天模板进行微调。 例如模型遇到这样的模板【问题-&gt;回答，问题-&gt;回答】后不再无脑接龙，而是意识到这是一段完整的对话结束。 称这个过程为指令微调。<br>&emsp;&emsp;<strong>训练数据为usft_mini_512.jsonl</strong><br><img src="/image/sft2.png" alt="训练集"><br><strong>训练命令：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">python train_full_sft.py<br></code></pre></td></tr></table></figure><p><strong>训练过程：</strong><br><img src="/image/sft1.png" alt="训练过程"><br><img src="/image/sft3.png" alt="训练过程"><br><strong>测试过程：</strong><br><img src="/image/sft4.png" alt="sft测试"><br><img src="/image/sft5.png" alt="sft测试"><br><img src="/image/sft6.png" alt="sft测试"><br>&emsp;&emsp;<strong>我们可以看到它在回答问题不会像之前预训练后，无脑的把一堆相关的内容输出，而是通过问题，给出合适的答案</strong></p><h2 id="5-LoRA"><a href="#5-LoRA" class="headerlink" title="5.LoRA"></a>5.LoRA</h2><p>&emsp;&emsp;LoRA是一种高效的参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）方法，旨在通过低秩分解的方式对预训练模型进行微调。 相比于全参数微调（Full Fine-Tuning），LoRA 只需要更新少量的参数。 LoRA 的核心思想是：在模型的权重矩阵中引入低秩分解，仅对低秩部分进行更新，而保持原始预训练权重不变。<br>&emsp;&emsp;通俗易懂来说，loRA ​资源消耗少​、 ​即插即用，像U盘（不破坏原始模型能力），防止灾难性遗忘（传统微调会覆盖原有知识，LoRA通过添加新”知识层”保留原始能力，类似在书上贴便签​）。<br><img src="/image/lora5.png" alt="各种训练比较"><br>&emsp;&emsp;<strong>训练数据（医学相关领域的知识）为lora_medical.jsonl</strong><br><img src="/image/lora3.png" alt="训练集"><br><img src="/image/lora4.png" alt="训练过程"><br><strong>测试过程：</strong><br><img src="/image/lora1.png" alt="测试内容"><br><img src="/image/lora2.png" alt="测试内容"><br>&emsp;&emsp;<strong>我们可以看到它在回答专业问题时，可以做到专业回答，同时也说明LORA适用于私人化知识模型定制。</strong></p><h2 id="6-训练推理模型"><a href="#6-训练推理模型" class="headerlink" title="6.训练推理模型"></a>6.训练推理模型</h2><p>&emsp;&emsp;<strong>DeepSeek-R1实在太火了，几乎重新指明了未来LLM的新范式。 但我还没有开始测试，只是下载了相关的数据集。</strong><br><img src="/image/deepseek1.png" alt="训练集"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">&#123;&quot;conversations&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请解释一下“读万卷书，行万里路”的含义。&quot;&#125;, &#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot; \n嗯，用户问的是“读万卷书，行万里路”的意思。首先我得想想这个短语出自哪里。好像以前听说过，可能是商鞅变法时期的名言，对吧？商鞅在秦国变法的时候用了这个短语，说明读书和行路一样重要。\n\n那我应该怎样解释呢？分几个方面来说明，可能包括读书的重要性，行路的意义，以及两者结合起来的效果。比如读书能开阔视野，行万里路能收获成功。\n\n可能用户是学生或者是对历史感兴趣的人，想了解这个成语的由来和用处。我应该简单介绍一下，让回答既准确又易懂。还要强调读书和行路的重要性，因为这两个看似无关的事物其实是分不开的。\n\n可能还要解释一下“万卷书”和“万里路”的含义，或者让用户明白为什么这两个词汇在这里Used.这样用户就能全面地理解这个成语的意思了。总之，答案应该简洁明了，同时带点历史背景，帮助用户更好地记忆和理解。\n \n \n“读万卷书，行万里路” 是一个很好的成语，具有深刻的寓意和历史背景。\n\n### 解释：\n1. 读万卷书：指的是广泛学习和积累知识。通过阅读大量书籍，可以开阔视野，增长见识，获得丰富的知识和深厚的学识。\n2. 行万里路：是指努力行动，依靠努力和积累来实现目标或达到目的。行万里路不仅包括远方的大规模行动，还包括远方的小规模行动，只要坚持不懈，最终都能带来成功。\n\n### 意义：\n- 这句话强调了知识的重要性，同时也强调了行动和努力。读万卷书可以让人获得广博的知识，但如果没有不畏艰难、不断努力的步伐，也无法实现远大的目标。\n- “万里路”不仅指远大的目标，也包括细微的努力和坚持。它提醒人们，要想成功，不仅需要Collection of knowledge，还需要直面困难、走出去、不懈奋斗。\n\n### 总结：\n“读书万卷，行路万里”强调了知识与行动的统一体，强调了通过勤奋和努力来实现目标的重要性。\n &quot;&#125;]&#125;   这是训练的一个部分<br></code></pre></td></tr></table></figure><p>&emsp;&emsp;<strong>我们可以看到DeepSeek-R1增加了think部分，就是深度思考的部分，这部分内容可以作为实战演练优化。由于时间问题，我还没有进行模型的训练。</strong></p><h2 id="7-知识蒸馏"><a href="#7-知识蒸馏" class="headerlink" title="7.知识蒸馏"></a>7.知识蒸馏</h2><p>&emsp;&emsp;知识蒸馏可以进一步优化模型的性能和效率，所谓知识蒸馏，即学生模型面向教师模型学习。 教师模型通常是经过充分训练的大模型，具有较高的准确性和泛化能力。 学生模型是一个较小的模型，目标是学习教师模型的行为，而不是直接从原始数据中学习。<br>&emsp;&emsp;在SFT学习中，模型的目标是拟合词Token分类硬标签（hard labels），即真实的类别标签（如 0 或 6400）。 在知识蒸馏中，教师模型的softmax概率分布被用作软标签（soft labels）。小模型仅学习软标签，并使用KL-Loss来优化模型的参数。<br>&emsp;&emsp;通俗地说，SFT直接学习老师给的解题答案。而KD过程相当于“打开”老师聪明的大脑，尽可能地模仿老师“大脑”思考问题的神经元状态。 例如，当老师模型计算1+1&#x3D;2这个问题的时候，最后一层神经元a状态为0，神经元b状态为100，神经元c状态为-99… 学生模型通过大量数据，学习教师模型大脑内部的运转规律。这个过程即称之为：知识蒸馏。<br>&emsp;&emsp;知识蒸馏的目的只有一个：让小模型体积更小的同时效果更好。 然而随着LLM诞生和发展，模型蒸馏一词被广泛滥用，从而产生了“白盒&#x2F;黑盒”知识蒸馏两个派别。 GPT-4这种闭源模型，由于无法获取其内部结构，因此只能面向它所输出的数据学习，这个过程称之为黑盒蒸馏，也是大模型时代最普遍的做法。 黑盒蒸馏与SFT过程完全一致，只不过数据是从大模型的输出收集，因此只需要准备数据并且进一步FT即可。 注意更改被加载的基础模型为full_sft_*.pth，即基于微调模型做进一步的蒸馏学习。<br>&emsp;&emsp;<strong>.&#x2F;dataset&#x2F;sft_1024.jsonl与.&#x2F;dataset&#x2F;sft_2048.jsonl 均收集自qwen2.5-7&#x2F;72B-Instruct大模型，可直接用于SFT以获取Qwen的部分行为。</strong><br><img src="/image/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F.png" alt="数据集"><br>&emsp;&emsp;<strong>由于时间问题，暂无测试。</strong></p>]]></content>
      
      
      <categories>
          
          <category> AI实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> 模型训练 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>说话人分离并转录实战</title>
      <link href="/2024/02/27/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5/"/>
      <url>/2024/02/27/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[<h1 id="服务器部署-FastAPI-说话人分离应用完整指南"><a href="#服务器部署-FastAPI-说话人分离应用完整指南" class="headerlink" title="服务器部署 FastAPI 说话人分离应用完整指南"></a>服务器部署 FastAPI 说话人分离应用完整指南</h1><h3 id="本项目基于FunASR开源制作"><a href="#本项目基于FunASR开源制作" class="headerlink" title="本项目基于FunASR开源制作"></a>本项目基于<a href="https://github.com/modelscope/FunASR">FunASR</a>开源制作</h3><h2 id="1-部署前准备"><a href="#1-部署前准备" class="headerlink" title="1.部署前准备"></a>1.部署前准备</h2><h3 id="1-1​服务器环境要求"><a href="#1-1​服务器环境要求" class="headerlink" title="1.1​服务器环境要求"></a>1.1​服务器环境要求</h3><p>&emsp;&emsp;Ubuntu 18.04   （也可本地部署成功后再部署到服务器重）<br>&emsp;&emsp;Python 3.8+<br>&emsp;&emsp;CUDA 11.8+（如需GPU加速）<br>&emsp;&emsp;至少 4GB 显存<br>&emsp;&emsp;存储空间：模型文件需预留 10GB+ 空间</p><h3 id="1-2​目录结构"><a href="#1-2​目录结构" class="headerlink" title="1.2​目录结构"></a>1.2​目录结构</h3><p><strong>bash<br>&#x2F;project-root<br>├── app<br>│   ├── AudioSeparation.py<br>│   ├── main.py<br>│   ├── requirements.txt<br>├── models<br>│   ├── paraformer-zh<br>│   ├── speech_fsmn_vad_zh-cn-16k-common-pytorch<br>│   └── …<br>├── static<br>│   └── styles.css        # 示例CSS文件<br>├── templates<br>│   └── upload.html<br>└── logs                  # 日志目录</strong></p><h2 id="2-AudioSeparationGUI（音频分离）"><a href="#2-AudioSeparationGUI（音频分离）" class="headerlink" title="2.AudioSeparationGUI（音频分离）"></a>2.AudioSeparationGUI（音频分离）</h2><p>&emsp;&emsp;AudioSeparationGUI（音频分离），是实现多人对话（如电话、会议等情景）的音频分离，大家可以体验一下。<br>  <img src="/image/test.jpg" alt="测试音频转录"><br>&emsp;&emsp;上面的结果还处于测试阶段，我们实际使用的时候（或给别人使用的时候）往往需要一个前端界面去操控，这个时候我们采用一种FasterApi框架去完成前端交互的任务。<br>&emsp;&emsp;FastAPi是一个现代、快速（高性能）的python web框架，客户端通过GET、POST、PUT、DELETE等动作，对服务器端资源进行操作。通俗来说服务器部署后，用户可以通过浏览器进行资源的访问。<br>&emsp;&emsp;我在部署前端界面增添了一些功能：<br>&emsp;&emsp;增添了前端可视化界面<br>&emsp;&emsp;增添了用户选择音频文件的可视化界面<br>&emsp;&emsp;增添了批量上传音频的功能<br>&emsp;&emsp;增添了下载后默认保存到浏览器下载路径，以便用户寻找转录结果。<br> <img src="/image/show1.jpg" alt="测试音频转录"><br> <img src="/image/show2.jpg" alt="测试音频转录"></p><h2 id="3-部署"><a href="#3-部署" class="headerlink" title="3.部署"></a>3.部署</h2><h3 id="3-1相关依赖下载"><a href="#3-1相关依赖下载" class="headerlink" title="3.1相关依赖下载"></a>3.1相关依赖下载</h3><p>&emsp;&emsp;根据requirements.txt提供的依赖下载。<em>（最好先创建一个虚拟环境或者用conda）</em><br>  执行命令：<code>pip install requirements</code></p><h3 id="requirements-txt"><a href="#requirements-txt" class="headerlink" title="requirements.txt"></a>requirements.txt</h3>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">torchvision<br>fastapi<br>uvicorn<br>torchaudio  <br>funasr==1.2.0  <br>modelscope  <br>huggingface<br>huggingface_hub<br>pydub<br>onnx<br>onnxconverter-common<br>ffmpeg-python<br></code></pre></td></tr></table></figure><h3 id="3-2相关模型的下载"><a href="#3-2相关模型的下载" class="headerlink" title="3.2相关模型的下载"></a>3.2相关模型的下载</h3><h3 id="modelscope（魔塔社区中下载）"><a href="#modelscope（魔塔社区中下载）" class="headerlink" title="modelscope（魔塔社区中下载）"></a>modelscope（魔塔社区中下载）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">speech_campplus_sv_zh-cn_16k-common<br>speech_fsmn_vad_zh-cn-16k-common-pytorch<br>speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch<br>punc_ct-transformer_zh-cn-common-vocab272727-pytorch<br></code></pre></td></tr></table></figure><h2 id="4-程序"><a href="#4-程序" class="headerlink" title="4.程序"></a>4.程序</h2><p>&emsp;&emsp;<strong>详细程序到GitHub上看看啦</strong><br>&emsp;&emsp;地址为：<a href="https://github.com/juge-ge/Speaker-Diarization">https://github.com/juge-ge/Speaker-Diarization</a></p><h2 id="5-服务器部署"><a href="#5-服务器部署" class="headerlink" title="5.服务器部署"></a>5.服务器部署</h2><p>&emsp;&emsp;登录服务器界面后   创建虚拟环境<br>&emsp;&emsp;下载好相关依赖<br>&emsp;&emsp;下载好相关模型<br>&emsp;&emsp;运行即可</p><p><strong>以下是服务器使用步骤：<br>1.启动前移动到该路径下     cd &#x2F;home&#x2F;data&#x2F;jdssy_liy&#x2F;Speaker\ Diarization（以我使用的服务器为例）<br>2.进入虚拟环境    source &#x2F;home&#x2F;data&#x2F;jdssy_liy&#x2F;my_venv&#x2F;bin&#x2F;activate<br>3.启动该程序   uvicorn main:app –host 0.0.0.0 –port 8090运行命令<br>4.使用结束后退出该环境   deactivate<br>以上是我在服务器部署的命令，大家可以根据自己调整，运行后输入服务器公网IP+端口即可访问</strong></p>]]></content>
      
      
      <categories>
          
          <category> AI实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 语音识别 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
  
</search>
